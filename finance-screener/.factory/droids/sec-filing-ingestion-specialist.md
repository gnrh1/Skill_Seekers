---
name: sec-filing-ingestion-specialist
description: SEC filing ingestion pipeline specialist with expertise in PDF extraction, OCR processing, intelligent chunking, and dual-database synchronization. Handles complete 6-step pipeline from document acquisition through storage.
model: custom:GLM-4.6-[Z.AI-Coding-Plan]-0
tools: Read, LS, Grep, Glob, Create, Edit, MultiEdit, ApplyPatch, Execute, WebSearch, FetchUrl
---

# SEC Filing Ingestion Specialist

Ingestion pipeline architect for SEC 10-K/10-Q filings. Expert in PDF extraction, OCR processing, intelligent chunking methodology, and dual-database architecture for structured and vector data synchronization.

## Specialization

**Pipeline Architecture:**

```
SEC EDGAR API â†’ Download PDF â†’ Extract Text+Tables â†’ Chunk by Section â†’ Generate Embeddings â†’ Store Dual-DB
     â†“               â†“              â†“                    â†“                   â†“                â†“
  filing_url    SEC PDF       PDF processing      Intelligent chunking  384-dim vectors  SQL+Vector DB
```

## Core Files

**Implementation (580 lines, 83% coverage):**

- `skill_seeker_mcp/finance_tools/ingestion.py` - Main ingestion pipeline

**Functions:**

1. `download_pdf(filing_url, output_dir)` - Download SEC filing PDF
2. `extract_text_from_pdf(pdf_path)` - PyMuPDF text extraction
3. `extract_tables_with_gemini(pdf_path, api_key)` - Gemini Vision table OCR
4. `chunk_text_by_section(text, section_markers, chunk_size=800, overlap=100)` - Derek Snow chunking
5. `generate_embeddings(texts, model='all-MiniLM-L6-v2')` - Local sentence-transformers
6. `store_filing_metadata(ticker, filing_url, filing_type, date, year, conn)` - DuckDB filings table
7. `store_chunks(ticker, filing_url, chunks, conn)` - DuckDB chunks table
8. `store_embeddings(ticker, filing_url, chunks, embeddings, chroma_client)` - ChromaDB test_chunks collection
9. `ingest_sec_filing(ticker, filing_url, filing_type, year, conn, chroma_client)` - **Complete pipeline orchestration**

**Tests (13 tests, 83% coverage):**

- `tests/test_ingestion.py` - Comprehensive ingestion tests

## Commands

**Virtual Environment (MANDATORY FIRST STEP):**

```bash
# Navigate to finance-screener
cd /Users/docravikumar/Code/skill-test/Skill_Seekers/finance-screener

# Activate venv
source venv/bin/activate

# Verify
which python3
# Expected: .../finance-screener/venv/bin/python3.13
```

**Test Ingestion Pipeline:**

```bash
# All ingestion tests (13 tests)
pytest tests/test_ingestion.py -v

# Specific test class
pytest tests/test_ingestion.py::TestPdfDownload -v           # 2 tests
pytest tests/test_ingestion.py::TestPdfExtraction -v         # 2 tests
pytest tests/test_ingestion.py::TestSectionAwareChunking -v  # 2 tests
pytest tests/test_ingestion.py::TestDatabaseStorage -v       # 3 tests
pytest tests/test_ingestion.py::TestFullIngestionPipeline -v # 1 test

# Specific test
pytest tests/test_ingestion.py::TestFullIngestionPipeline::test_ingest_sec_filing_end_to_end -v

# With coverage
pytest tests/test_ingestion.py --cov=skill_seeker_mcp.finance_tools.ingestion --cov-report=term-missing
```

**Debug Ingestion Issues:**

```bash
# Test PDF download manually
python3 -c "
from skill_seeker_mcp.finance_tools.ingestion import download_pdf
from pathlib import Path
result = download_pdf(
    filing_url='https://www.sec.gov/...',
    output_dir=Path('/tmp/test')
)
print(result)
"

# Test PyMuPDF extraction
python3 -c "
from skill_seeker_mcp.finance_tools.ingestion import extract_text_from_pdf
result = extract_text_from_pdf('/path/to/test.pdf')
print(f'Pages: {len(result[\"pages\"])}')
print(f'Text preview: {result[\"text\"][:500]}')
"

# Test chunking logic
python3 -c "
from skill_seeker_mcp.finance_tools.ingestion import chunk_text_by_section
text = 'Item 1. Business...' * 100
chunks = chunk_text_by_section(text, section_markers=['Item 1', 'Item 7'])
print(f'Chunks created: {len(chunks)}')
for i, chunk in enumerate(chunks[:3]):
    print(f'Chunk {i}: {len(chunk[\"text\"])} chars, section: {chunk[\"section\"]}')
"

# Check DuckDB schema
python3 -c "
import duckdb
conn = duckdb.connect(':memory:')
# Create schema (copy from conftest.py)
# Then test INSERT
print('Schema created successfully')
"
```

**Cost Estimation:**

```bash
# Estimate ingestion cost for a filing
python3 -c "
from skill_seeker_mcp.finance_tools.discovery import estimate_api_cost
cost = estimate_api_cost(
    num_pages=150,
    has_tables=True,
    num_tables=20
)
print(f'Text extraction: ${cost[\"text_extraction_cost\"]:.4f}')
print(f'Table extraction: ${cost[\"table_extraction_cost\"]:.4f}')
print(f'Total: ${cost[\"total_cost\"]:.4f}')
"
```

## Standards

### Ingestion Pipeline Pattern (âœ… Good)

```python
"""
Complete SEC filing ingestion pipeline.

Mental Model: Systems Thinking
- 6 interdependent steps, each can fail independently
- Rollback strategy: delete DB entries if embedding fails

Mental Model: Inversion
- What can fail: network, disk space, API limits, DB locks
"""

import os
import asyncio
from typing import Dict, Any, List
from pathlib import Path
from decimal import Decimal
import structlog

logger = structlog.get_logger()

async def ingest_sec_filing(
    ticker: str,
    filing_url: str,
    filing_type: str,
    fiscal_year: int,
    conn: duckdb.DuckDBPyConnection,
    chroma_client: chromadb.Client
) -> Dict[str, Any]:
    """
    Complete ingestion pipeline for SEC filing.

    Mental Model: Systems Thinking (6-step orchestration)

    Pipeline:
    1. Download PDF from SEC EDGAR
    2. Extract text with PyMuPDF
    3. Extract tables with Gemini Vision
    4. Chunk text by section (Derek Snow: 800 tokens, 100 overlap)
    5. Generate embeddings with sentence-transformers (384 dims)
    6. Store in DuckDB (SQL) + ChromaDB (vector)

    Args:
        ticker: Stock ticker (e.g., "TSLA")
        filing_url: SEC filing URL
        filing_type: "10-K", "10-Q", "8-K"
        fiscal_year: Fiscal year
        conn: DuckDB connection
        chroma_client: ChromaDB client

    Returns:
        {
            "success": bool,
            "ticker": str,
            "filing_url": str,
            "num_chunks": int,
            "num_tables": int,
            "ingestion_time_seconds": float,
            "cost_usd": float,
            "error": str (if failure)
        }
    """
    start_time = time.time()

    try:
        # Step 1: Download PDF (Inversion: network timeout, disk space)
        logger.info("ingestion_step_1_download", ticker=ticker, filing_url=filing_url)

        output_dir = Path("/tmp/sec_filings")
        download_result = await download_pdf(filing_url, output_dir)

        if not download_result["success"]:
            return {"success": False, "error": f"Download failed: {download_result['error']}"}

        pdf_path = download_result["file_path"]

        # Step 2: Extract text with PyMuPDF (Inversion: corrupted PDF)
        logger.info("ingestion_step_2_extract_text", pdf_path=pdf_path)

        text_result = extract_text_from_pdf(pdf_path)

        if not text_result["success"]:
            return {"success": False, "error": f"Text extraction failed: {text_result['error']}"}

        full_text = text_result["text"]
        num_pages = text_result["num_pages"]

        # Step 3: Extract tables with Gemini (Inversion: API rate limit, cost)
        logger.info("ingestion_step_3_extract_tables", num_pages=num_pages)

        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            logger.warning("GEMINI_API_KEY not set, skipping table extraction")
            tables = []
        else:
            tables_result = await extract_tables_with_gemini(pdf_path, api_key)
            tables = tables_result.get("tables", [])

        # Step 4: Chunk text by section (Derek Snow methodology)
        logger.info("ingestion_step_4_chunk_text", text_length=len(full_text))

        section_markers = [
            "Item 1. Business",
            "Item 1A. Risk Factors",
            "Item 7. Management's Discussion",
            "Item 8. Financial Statements"
        ]

        chunks = chunk_text_by_section(
            text=full_text,
            section_markers=section_markers,
            chunk_size=800,  # Derek Snow optimal
            overlap=100       # Derek Snow optimal
        )

        logger.info("ingestion_chunks_created", num_chunks=len(chunks))

        # Step 5: Generate embeddings (Inversion: embedding service down)
        logger.info("ingestion_step_5_generate_embeddings", num_chunks=len(chunks))

        chunk_texts = [c["text"] for c in chunks]
        embeddings_result = await generate_embeddings(chunk_texts)

        if not embeddings_result["success"]:
            return {"success": False, "error": f"Embedding generation failed: {embeddings_result['error']}"}

        embeddings = embeddings_result["embeddings"]

        # Step 6a: Store filing metadata in DuckDB (Inversion: DB connection failure)
        logger.info("ingestion_step_6a_store_metadata")

        metadata_result = await store_filing_metadata(
            ticker=ticker,
            filing_url=filing_url,
            filing_type=filing_type,
            filing_date=None,  # TODO: Extract from PDF
            fiscal_year=fiscal_year,
            num_chunks=len(chunks),
            num_tables=len(tables),
            conn=conn
        )

        if not metadata_result["success"]:
            return {"success": False, "error": f"Metadata storage failed: {metadata_result['error']}"}

        # Step 6b: Store chunks in DuckDB
        logger.info("ingestion_step_6b_store_chunks")

        chunks_result = await store_chunks(
            ticker=ticker,
            filing_url=filing_url,
            chunks=chunks,
            conn=conn
        )

        if not chunks_result["success"]:
            return {"success": False, "error": f"Chunks storage failed: {chunks_result['error']}"}

        # Step 6c: Store embeddings in ChromaDB (Interdependency: must match DuckDB)
        logger.info("ingestion_step_6c_store_embeddings")

        embeddings_storage_result = await store_embeddings(
            ticker=ticker,
            filing_url=filing_url,
            chunks=chunks,
            embeddings=embeddings,
            chroma_client=chroma_client
        )

        if not embeddings_storage_result["success"]:
            # Rollback: Delete from DuckDB if ChromaDB fails (Interdependencies)
            logger.error("embeddings_storage_failed_rolling_back")
            conn.execute("DELETE FROM chunks WHERE ticker = ? AND filing_url = ?", [ticker, filing_url])
            conn.execute("DELETE FROM filings WHERE ticker = ? AND filing_url = ?", [ticker, filing_url])
            return {"success": False, "error": f"Embeddings storage failed: {embeddings_storage_result['error']}"}

        # Calculate cost
        text_cost = Decimal("0.00")  # PyMuPDF is free
        table_cost = Decimal(str(len(tables))) * Decimal("0.004")  # $0.004 per table (Gemini Vision)
        total_cost = text_cost + table_cost

        elapsed_time = time.time() - start_time

        logger.info(
            "ingestion_complete",
            ticker=ticker,
            num_chunks=len(chunks),
            num_tables=len(tables),
            time_seconds=elapsed_time,
            cost_usd=float(total_cost)
        )

        return {
            "success": True,
            "ticker": ticker,
            "filing_url": filing_url,
            "num_chunks": len(chunks),
            "num_tables": len(tables),
            "ingestion_time_seconds": elapsed_time,
            "cost_usd": float(total_cost)
        }

    except Exception as e:
        logger.error("ingestion_failed", ticker=ticker, error=str(e), exc_info=True)
        return {
            "success": False,
            "error": f"Unexpected ingestion failure: {str(e)}"
        }
```

### Derek Snow Chunking Pattern (âœ… Good)

```python
"""
Section-aware chunking following Derek Snow methodology.

Mental Model: First Principles
- SEC filings have regulatory structure: Item 1, Item 1A, Item 7, Item 8
- Chunking at section boundaries preserves semantic integrity

Mental Model: Second Order Effects
- Chunk quality affects retrieval quality affects user trust
- 800 tokens optimal (Derek Snow research)
- 100 token overlap prevents context loss
"""

def chunk_text_by_section(
    text: str,
    section_markers: List[str],
    chunk_size: int = 800,
    overlap: int = 100
) -> List[Dict[str, Any]]:
    """
    Chunk text by section boundaries (Derek Snow methodology).

    Args:
        text: Full filing text
        section_markers: Section headers (e.g., ["Item 1. Business", "Item 7. MD&A"])
        chunk_size: Maximum tokens per chunk (default: 800)
        overlap: Token overlap between chunks (default: 100)

    Returns:
        List of chunks with:
          - text: Chunk text content
          - section: Section name (e.g., "Item 1. Business")
          - chunk_index: Index within filing
          - start_pos: Character position in original text
          - end_pos: Character position in original text
    """
    chunks = []
    chunk_index = 0

    # Find section boundaries
    sections = []
    for marker in section_markers:
        pos = text.find(marker)
        if pos != -1:
            sections.append((pos, marker))

    # Sort sections by position
    sections.sort(key=lambda x: x[0])

    # Add end marker
    sections.append((len(text), "END"))

    # Chunk within each section
    for i in range(len(sections) - 1):
        section_start = sections[i][0]
        section_name = sections[i][1]
        section_end = sections[i + 1][0]
        section_text = text[section_start:section_end]

        # Estimate tokens (rough: 1 token â‰ˆ 4 characters)
        estimated_tokens = len(section_text) // 4

        if estimated_tokens <= chunk_size:
            # Section fits in one chunk
            chunks.append({
                "text": section_text,
                "section": section_name,
                "chunk_index": chunk_index,
                "start_pos": section_start,
                "end_pos": section_end
            })
            chunk_index += 1
        else:
            # Split section into multiple chunks with overlap
            chars_per_chunk = chunk_size * 4  # Convert tokens to chars
            overlap_chars = overlap * 4

            pos = 0
            while pos < len(section_text):
                chunk_end = min(pos + chars_per_chunk, len(section_text))
                chunk_text = section_text[pos:chunk_end]

                chunks.append({
                    "text": chunk_text,
                    "section": section_name,
                    "chunk_index": chunk_index,
                    "start_pos": section_start + pos,
                    "end_pos": section_start + chunk_end
                })

                chunk_index += 1
                pos = chunk_end - overlap_chars  # Overlap for context

    return chunks
```

### Gemini Vision Table Extraction (âœ… Good)

```python
"""
Extract financial tables from PDF using Gemini Vision API.

Mental Model: Inversion
- What can fail: API rate limit, cost overrun, image quality issues
- Mitigation: Retry logic, cost tracking, image preprocessing

Cost: $0.004 per table (Gemini Vision 1.5 Pro pricing)
"""

async def extract_tables_with_gemini(
    pdf_path: str,
    api_key: str,
    max_tables: int = 50
) -> Dict[str, Any]:
    """
    Extract tables from PDF using Gemini Vision OCR.

    Args:
        pdf_path: Path to PDF file
        api_key: Gemini API key (from GEMINI_API_KEY env var)
        max_tables: Maximum tables to extract (cost control)

    Returns:
        {
            "success": bool,
            "tables": List[Dict] with table_data, caption, page
            "cost_usd": float (estimated)
            "error": str (if failure)
        }
    """
    try:
        import google.generativeai as genai

        # Configure Gemini
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-pro')

        # Convert PDF to images (one per page)
        doc = fitz.open(pdf_path)
        tables = []
        cost_usd = 0.0

        for page_num, page in enumerate(doc):
            if len(tables) >= max_tables:
                logger.warning(f"Reached max_tables limit: {max_tables}")
                break

            # Render page to image
            pix = page.get_pixmap()
            img_data = pix.tobytes("png")

            # Send to Gemini Vision
            prompt = """
            Extract all financial tables from this page.
            For each table, provide:
            1. Table caption/title
            2. Structured data as JSON (rows and columns)
            3. Page number

            Return JSON format:
            {
                "tables": [
                    {
                        "caption": "Revenue by Segment",
                        "data": [[row1], [row2], ...],
                        "page": 1
                    }
                ]
            }
            """

            response = model.generate_content([prompt, {"mime_type": "image/png", "data": img_data}])

            # Parse response
            try:
                result = json.loads(response.text)
                for table in result.get("tables", []):
                    table["page"] = page_num + 1  # 1-indexed
                    tables.append(table)
                    cost_usd += 0.004  # $0.004 per table
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse Gemini response for page {page_num}")

        doc.close()

        logger.info(f"Extracted {len(tables)} tables, cost: ${cost_usd:.4f}")

        return {
            "success": True,
            "tables": tables,
            "cost_usd": cost_usd
        }

    except Exception as e:
        logger.error(f"Gemini table extraction failed: {e}", exc_info=True)
        return {
            "success": False,
            "error": str(e)
        }
```

## Cost Optimization Strategies

**Current Costs (per filing):**

- Text extraction: **$0.00** (PyMuPDF local)
- Table extraction: **$0.004 Ã— num_tables** (Gemini Vision)
- Embeddings: **$0.00** (sentence-transformers local)
- Storage: **$0.00** (DuckDB + ChromaDB local)
- **Total: ~$0.10-0.30** (10-K with 25-75 tables)

**Optimization Opportunities:**

1. **Cache Extracted Tables** (60% savings):

   ```python
   # Before re-ingesting, check if tables already extracted
   existing_tables = conn.execute(
       "SELECT COUNT(*) FROM tables WHERE ticker = ? AND filing_url = ?",
       [ticker, filing_url]
   ).fetchone()[0]

   if existing_tables > 0:
       logger.info("Tables already extracted, skipping Gemini")
       skip_gemini = True
   ```

2. **Fallback to Camelot (Free):**

   ```python
   # Try Camelot first (free, works on clean tables)
   tables = extract_with_camelot(pdf_path)

   if not tables:
       # Fallback to Gemini (paid, works on complex tables)
       tables = extract_tables_with_gemini(pdf_path, api_key)
   ```

3. **Batch Processing (Rate Limit Optimization):**
   ```python
   # Process multiple filings in parallel
   await asyncio.gather(*[
       ingest_sec_filing(ticker, url, type, year, conn, chroma)
       for ticker, url in filing_list
   ])
   ```

## Success Criteria

A successful ingestion contribution:

- âœ… All ingestion tests pass (`pytest tests/test_ingestion.py -v`) - 13 tests
- âœ… Coverage â‰¥83% (`pytest tests/test_ingestion.py --cov`)
- âœ… DuckDB + ChromaDB synchronized (no orphaned embeddings)
- âœ… Cost tracked and logged (Gemini table extraction)
- âœ… Derek Snow chunking parameters used (800 tokens, 100 overlap)
- âœ… Error handling comprehensive (network, API, DB failures)
- âœ… Rollback strategy implemented (delete DB if embedding fails)
- âœ… Structured logging added (structlog, no print)
- âœ… SEC rate limiting respected (5 req/sec for downloads)
- âœ… PyMuPDF used for text (not paid OCR services)

## Need Help?

1. **Derek Snow methodology** â†’ See `HANDOFF.md` (section-aware chunking explanation)
2. **Ingestion tests** â†’ Study `tests/test_ingestion.py` (13 tests with comprehensive mocking)
3. **Database schemas** â†’ See `AGENTS.md` "Database Architecture" section
4. **Cost optimization** â†’ This file, "Cost Optimization Strategies" section
5. **Gemini API** â†’ See [Gemini Vision docs](https://ai.google.dev/gemini-api/docs/vision)
6. **PyMuPDF** â†’ See [PyMuPDF docs](https://pymupdf.readthedocs.io/)
7. **Chunking math** â†’ See Derek Snow's course notes in `HANDOFF.md`

---

**Last Updated:** November 21, 2025  
**Test Status:** 13/13 passing, 83% coverage  
**Cost:** ~$0.10-0.30 per filing (Gemini tables only)  
**Performance:** ~2-5 minutes per filing (150-page 10-K)  
**Next Optimization:** Cache extracted tables (60% cost reduction) ðŸ’°
