#!/usr/bin/env python3
"""
EXTREME Memory Stress Test Agent
Pushes memory management to absolute limits with pathological edge cases

Focus Areas:
1. Deep nesting data structures (2000+ levels)
2. Extreme string operations and concatenations
3. Complex circular references and object graphs
4. High-frequency allocation/deallocation cycles
5. Concurrent memory access race conditions
6. Memory corruption detection
7. Resource exhaustion recovery

Runtime: 30 seconds of extreme stress testing
"""

import gc
import os
import sys
import time
import psutil
import threading
import random
import weakref
import traceback
import json
import multiprocessing
import array
import mmap
import tempfile
import itertools
import collections
import string
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import ctypes

class ExtremeMemoryStressTestAgent:
    """Extreme memory stress testing for pathological edge cases"""

    def __init__(self):
        self.process = psutil.Process()
        self.start_time = time.time()
        self.test_duration = 30  # seconds
        self.results = {
            'deep_nesting': {},
            'extreme_strings': {},
            'complex_circular_refs': {},
            'allocation_cycles': {},
            'race_conditions': {},
            'memory_corruption': {},
            'resource_exhaustion': {}
        }

        # Extreme test configuration
        self.deep_nesting_levels = 2500  # Extreme depth
        self.string_test_size = 10000000  # 10M character strings
        self.allocation_cycles = 10000  # High frequency cycles
        self.concurrent_threads = 20  # High concurrency
        self.corruption_iterations = 5000

    def get_memory_stats(self) -> Dict[str, Any]:
        """Get comprehensive memory statistics"""
        try:
            memory_info = self.process.memory_info()
            memory_full = self.process.memory_full_info()

            return {
                'rss_mb': memory_info.rss / (1024 * 1024),
                'vms_mb': memory_info.vms / (1024 * 1024),
                'shared_mb': getattr(memory_full, 'shared', 0) / (1024 * 1024),
                'uss_mb': getattr(memory_full, 'uss', 0) / (1024 * 1024),
                'percent': self.process.memory_percent(),
                'available_mb': psutil.virtual_memory().available / (1024 * 1024),
                'timestamp': time.time(),
                'gc_stats': {
                    'counts': gc.get_count(),
                    'collectable': len(gc.garbage),
                    'thresholds': gc.get_threshold()
                }
            }
        except Exception as e:
            return {'error': str(e)}

    def test_extreme_deep_nesting(self) -> Dict[str, Any]:
        """Test 1: Extreme deep nesting data structures"""
        print("\nüèóÔ∏è  Testing EXTREME Deep Nesting...")

        initial_memory = self.get_memory_stats()
        nested_objects = []
        corruption_detected = 0
        max_depth_achieved = 0

        try:
            print("  ‚Ä¢ Creating deeply nested structures (2500 levels)...")

            def create_nested_structure(depth, current_depth=0):
                """Recursively create deeply nested structures"""
                nonlocal max_depth_achieved
                max_depth_achieved = max(max_depth_achieved, current_depth)

                if depth <= 0:
                    return {'leaf': True, 'depth': current_depth}

                # Create nested object with mixed data types
                obj = {
                    'depth': current_depth,
                    'data': list(range(10)),
                    'string': f'nested_level_{current_depth}',
                    'array': array.array('i', range(5)),
                    'nested': create_nested_structure(depth - 1, current_depth + 1)
                }

                # Add some circular references to make it interesting
                if current_depth % 100 == 0:
                    obj['circular_ref'] = obj  # Self-reference at certain depths

                return obj

            # Create multiple deeply nested structures
            for i in range(5):  # Create 5 super-deep structures
                print(f"    Structure {i+1}/5...")
                nested_obj = create_nested_structure(self.deep_nesting_levels)
                nested_objects.append(nested_obj)

                # Test corruption detection
                if i > 0:
                    try:
                        # Try to access deep nested data
                        current = nested_obj
                        for _ in range(100):
                            if 'nested' in current and isinstance(current['nested'], dict):
                                current = current['nested']
                            else:
                                break

                        # Verify integrity
                        if 'depth' not in current:
                            corruption_detected += 1

                    except (RecursionError, MemoryError, KeyError):
                        # Expected at extreme depths
                        pass

            # Test 1.2: Recursive list nesting
            print("  ‚Ä¢ Creating recursively nested lists...")
            list_nesting = []
            current_list = list_nesting

            for i in range(1000):  # 1000 levels of list nesting
                next_list = []
                current_list.append(next_list)
                current_list = next_list

                # Add some data at each level
                current_list.extend([i, f'level_{i}', {'level': i}])

            # Test 1.3: Dictionary nesting with mixed keys
            print("  ‚Ä¢ Creating deeply nested dictionaries...")
            dict_nesting = {}
            current_dict = dict_nesting

            for i in range(800):  # 800 levels of dict nesting
                key_types = [
                    f'string_key_{i}',
                    i,  # integer key
                    (i, i+1),  # tuple key
                    frozenset([i, i+1])  # frozenset key
                ]

                next_dict = {}
                for key_type in key_types:
                    current_dict[key_type] = next_dict
                current_dict = next_dict

            peak_memory = self.get_memory_stats()

            # Test 1.4: Deep recursion stress test
            print("  ‚Ä¢ Testing deep recursion access patterns...")
            recursion_errors = 0

            def deep_traversal(obj, max_depth=1000):
                """Traverse deeply nested structure"""
                depth = 0
                try:
                    while isinstance(obj, dict) and 'nested' in obj and depth < max_depth:
                        obj = obj['nested']
                        depth += 1

                        # Access some data
                        _ = obj.get('depth', 0)
                        _ = obj.get('data', [])

                except (RecursionError, AttributeError, KeyError):
                    return -1  # Error
                return depth

            # Test traversal on all objects
            traversal_depths = []
            for obj in nested_objects[:3]:  # Test first 3 to avoid excessive time
                depth = deep_traversal(obj, 500)  # Limit traversal depth
                if depth >= 0:
                    traversal_depths.append(depth)
                else:
                    recursion_errors += 1

            final_memory = self.get_memory_stats()
            memory_growth = final_memory['rss_mb'] - initial_memory['rss_mb']

            return {
                'initial_rss_mb': initial_memory['rss_mb'],
                'peak_rss_mb': peak_memory['rss_mb'],
                'final_rss_mb': final_memory['rss_mb'],
                'memory_growth_mb': memory_growth,
                'nested_structures_created': len(nested_objects),
                'max_nesting_depth': max_depth_achieved,
                'list_nesting_levels': 1000,
                'dict_nesting_levels': 800,
                'corruption_detected': corruption_detected,
                'recursion_errors': recursion_errors,
                'avg_traversal_depth': sum(traversal_depths) / len(traversal_depths) if traversal_depths else 0,
                'deep_nesting_successful': max_depth_achieved >= 2000,
                'memory_efficient': memory_growth < 500  # Less than 500MB is efficient
            }

        except Exception as e:
            return {'error': str(e), 'traceback': traceback.format_exc()}

    def test_extreme_string_operations(self) -> Dict[str, Any]:
        """Test 2: Extreme string operations and concatenations"""
        print("\nüìù Testing EXTREME String Operations...")

        initial_memory = self.get_memory_stats()
        strings_created = []
        corruption_count = 0

        try:
            print("  ‚Ä¢ Creating massive string operations...")

            # Test 2.1: Large string concatenations
            print("    ‚Ä¢ Massive concatenations...")
            base_string = "x" * 1000000  # 1MB base string

            concatenation_times = []
            concatenated_string = ""

            for i in range(10):  # 10 concatenations of 1MB each
                start_time = time.time()
                concatenated_string += base_string
                end_time = time.time()
                concatenation_times.append(end_time - start_time)

                if i % 2 == 0:
                    # Verify integrity at checkpoints
                    expected_length = (i + 1) * 1000000
                    if len(concatenated_string) != expected_length:
                        corruption_count += 1

            # Test 2.2: String multiplication stress
            print("    ‚Ä¢ String multiplication stress...")
            multiplication_results = []

            for multiplier in [1000, 5000, 10000, 50000]:
                try:
                    test_string = "y" * 1000
                    start_time = time.time()
                    multiplied = test_string * multiplier
                    end_time = time.time()

                    multiplication_results.append({
                        'multiplier': multiplier,
                        'result_length': len(multiplied),
                        'time_ms': (end_time - start_time) * 1000,
                        'success': True
                    })

                    # Verify some data
                    sample = multiplied[:100]
                    if len(sample) != 100 or not all(c == 'y' for c in sample):
                        corruption_count += 1

                except MemoryError:
                    multiplication_results.append({
                        'multiplier': multiplier,
                        'result_length': 0,
                        'time_ms': 0,
                        'success': False,
                        'error': 'MemoryError'
                    })
                    break

            # Test 2.3: String encoding/decoding stress
            print("    ‚Ä¢ Encoding/decoding stress...")
            encoding_results = []

            test_string = "a" * 1000000  # 1MB test string
            encodings = ['utf-8', 'utf-16', 'utf-32', 'latin-1', 'ascii']

            for encoding in encodings:
                try:
                    # Encode
                    start_time = time.time()
                    encoded = test_string.encode(encoding)
                    encode_time = time.time() - start_time

                    # Decode
                    start_time = time.time()
                    decoded = encoded.decode(encoding)
                    decode_time = time.time() - start_time

                    # Verify integrity
                    integrity_check = decoded == test_string

                    encoding_results.append({
                        'encoding': encoding,
                        'encoded_size': len(encoded),
                        'encode_time_ms': encode_time * 1000,
                        'decode_time_ms': decode_time * 1000,
                        'integrity_check': integrity_check
                    })

                    if not integrity_check:
                        corruption_count += 1

                except Exception as e:
                    encoding_results.append({
                        'encoding': encoding,
                        'error': str(e)
                    })

            # Test 2.4: Regular expression on massive strings
            print("    ‚Ä¢ Regex stress on massive strings...")
            import re

            massive_string = "test_pattern_" + "x" * 5000000 + "_end_test_pattern"  # 5MB string

            regex_patterns = [
                r'test_pattern_.*?_end_test_pattern',
                r'x+',
                r'.{1000,}',
                r'pattern.*'
            ]

            regex_results = []
            for pattern in regex_patterns:
                try:
                    start_time = time.time()
                    matches = re.findall(pattern, massive_string)
                    end_time = time.time()

                    regex_results.append({
                        'pattern': pattern,
                        'matches_found': len(matches),
                        'time_ms': (end_time - start_time) * 1000,
                        'success': True
                    })

                except Exception as e:
                    regex_results.append({
                        'pattern': pattern,
                        'error': str(e),
                        'success': False
                    })

            # Test 2.5: String formatting stress
            print("    ‚Ä¢ String formatting stress...")
            formatting_times = []

            large_template = "Value: {value}, List: {lst}, Dict: {dct}, Nested: {nested}"

            for i in range(1000):
                large_data = {
                    'value': 'x' * 1000,
                    'lst': list(range(100)),
                    'dct': {f'key_{j}': j for j in range(50)},
                    'nested': {'level1': {'level2': {'level3': list(range(20))}}}
                }

                start_time = time.time()
                formatted = large_template.format(**large_data)
                end_time = time.time()

                formatting_times.append(end_time - start_time)

                if i % 100 == 0 and '{value}' not in formatted:
                    corruption_count += 1

            peak_memory = self.get_memory_stats()

            # Cleanup some large strings to test memory recovery
            del concatenated_string
            del massive_string
            gc.collect()

            final_memory = self.get_memory_stats()
            memory_recovered = peak_memory['rss_mb'] - final_memory['rss_mb']

            return {
                'initial_rss_mb': initial_memory['rss_mb'],
                'peak_rss_mb': peak_memory['rss_mb'],
                'final_rss_mb': final_memory['rss_mb'],
                'memory_recovered_mb': memory_recovered,
                'base_string_size_mb': len(base_string) / (1024 * 1024),
                'concatenation_operations': len(concatenation_times),
                'avg_concatenation_time_ms': sum(concatenation_times) / len(concatenation_times) * 1000,
                'multiplication_tests': len(multiplication_results),
                'encoding_tests': len(encoding_results),
                'regex_tests': len(regex_results),
                'formatting_operations': len(formatting_times),
                'avg_formatting_time_ms': sum(formatting_times) / len(formatting_times) * 1000,
                'corruption_detected': corruption_count,
                'string_operations_successful': corruption_count == 0
            }

        except Exception as e:
            return {'error': str(e), 'traceback': traceback.format_exc()}

    def test_complex_circular_references(self) -> Dict[str, Any]:
        """Test 3: Complex circular references and object graphs"""
        print("\nüîÑ Testing Complex Circular References...")

        initial_memory = self.get_memory_stats()
        circular_objects = []
        weak_refs = []

        try:
            print("  ‚Ä¢ Creating complex object graphs with circular references...")

            # Test 3.1: Multi-level circular reference graphs
            def create_circular_graph(node_count):
                """Create a complex graph with circular references"""
                nodes = []

                class GraphNode:
                    def __init__(self, id):
                        self.id = id
                        self.references = []
                        self.data = list(range(id % 10))
                        self.metadata = {'created_at': time.time(), 'id': id}

                # Create nodes
                for i in range(node_count):
                    node = GraphNode(i)
                    nodes.append(node)
                    weak_refs.append(weakref.ref(node))

                # Create complex reference patterns
                for i, node in enumerate(nodes):
                    # Forward references
                    node.references.append(nodes[(i + 1) % len(nodes)])

                    # Backward references (creates cycles)
                    if i > 0:
                        node.references.append(nodes[i - 1])

                    # Random cross-references for complexity
                    if i % 3 == 0:
                        random_target = nodes[random.randint(0, len(nodes) - 1)]
                        node.references.append(random_target)

                    # Self-reference at specific intervals
                    if i % 10 == 0:
                        node.references.append(node)

                # Add some complex cycles
                for i in range(0, len(nodes) - 3, 4):
                    nodes[i].references.append(nodes[i + 2])
                    nodes[i + 2].references.append(nodes[i + 1])
                    nodes[i + 1].references.append(nodes[i + 3])
                    nodes[i + 3].references.append(nodes[i])

                return nodes

            # Create multiple circular graphs
            for graph_size in [100, 200, 300]:
                print(f"    ‚Ä¢ Creating graph with {graph_size} nodes...")
                graph_nodes = create_circular_graph(graph_size)
                circular_objects.extend(graph_nodes)

                # Test graph traversal
                visited_count = 0
                try:
                    def traverse_graph(node, visited=None):
                        nonlocal visited_count
                        if visited is None:
                            visited = set()

                        if id(node) in visited:
                            return

                        visited.add(id(node))
                        visited_count += 1

                        for ref in node.references:
                            traverse_graph(ref, visited.copy())

                    # Start traversal from first node
                    traverse_graph(graph_nodes[0])

                except RecursionError:
                    # Expected with complex cycles
                    pass

            # Test 3.2: Container circular references
            print("  ‚Ä¢ Creating container-based circular references...")

            for container_type in ['list', 'dict', 'set']:
                if container_type == 'list':
                    container1 = []
                    container2 = []
                    container1.append(container2)
                    container2.append(container1)
                    container1.append({'nested': container1})

                elif container_type == 'dict':
                    container1 = {}
                    container2 = {}
                    container1['ref'] = container2
                    container2['back_ref'] = container1
                    container1['self_ref'] = container1

                else:  # set (using frozenset for hashability)
                    frozenset1 = frozenset([1, 2, 3])
                    frozenset2 = frozenset([frozenset1])
                    container1 = {frozenset1, frozenset2}
                    container2 = {frozenset2, frozenset1}

                circular_objects.extend([container1, container2])

            # Test 3.3: Class instance circular references
            print("  ‚Ä¢ Creating class instance circular references...")

            class CircularParent:
                def __init__(self, name):
                    self.name = name
                    self.children = []
                    self.metadata = {'type': 'parent', 'id': id(self)}

                def add_child(self, child):
                    self.children.append(child)
                    child.parent = self
                    child.siblings = self.children

            class CircularChild:
                def __init__(self, name):
                    self.name = name
                    self.parent = None
                    self.siblings = []
                    self.metadata = {'type': 'child', 'id': id(self)}

            # Create family trees with circular references
            for i in range(50):
                parent = CircularParent(f'parent_{i}')
                children = [CircularChild(f'child_{i}_{j}') for j in range(3)]

                for child in children:
                    parent.add_child(child)

                # Add cross-family references
                if i > 0:
                    parent.children[0].cousin = circular_objects[-2]  # Reference to previous family

                circular_objects.extend([parent] + children)

            # Test 3.4: Generator and iterator circular references
            print("  ‚Ä¢ Testing generator circular references...")

            def circular_generator():
                """Generator that maintains circular reference"""
                data = ['generator_data'] * 100

                class IteratorWrapper:
                    def __init__(self, generator):
                        self.generator = generator
                        self.data_ref = data  # Circular reference

                    def __iter__(self):
                        return self.generator

                def generator_func():
                    for i in range(100):
                        yield f'item_{i}'

                gen = generator_func()
                wrapper = IteratorWrapper(gen)
                return wrapper

            # Create circular generators
            for i in range(20):
                gen_wrapper = circular_generator()
                circular_objects.append(gen_wrapper)

            # Test garbage collection effectiveness
            print("  ‚Ä¢ Testing garbage collection on complex circular references...")

            # Count weak references before GC
            pre_gc_surviving = sum(1 for ref in weak_refs if ref() is not None)

            # Force garbage collection multiple times
            for _ in range(3):
                gc.collect()

            # Count weak references after GC
            post_gc_surviving = sum(1 for ref in weak_refs if ref() is not None)

            peak_memory = self.get_memory_stats()

            # Test 3.5: Memory corruption detection
            print("  ‚Ä¢ Testing memory corruption detection...")
            corruption_detected = 0

            # Test integrity of circular structures
            sample_size = min(100, len(circular_objects))
            for i, obj in enumerate(circular_objects[:sample_size]):
                try:
                    # Basic integrity check
                    if hasattr(obj, '__dict__'):
                        _ = obj.__dict__
                    if hasattr(obj, '__len__'):
                        _ = len(obj)

                except Exception:
                    corruption_detected += 1

            final_memory = self.get_memory_stats()
            memory_growth = final_memory['rss_mb'] - initial_memory['rss_mb']

            return {
                'initial_rss_mb': initial_memory['rss_mb'],
                'peak_rss_mb': peak_memory['rss_mb'],
                'final_rss_mb': final_memory['rss_mb'],
                'memory_growth_mb': memory_growth,
                'circular_objects_created': len(circular_objects),
                'weak_refs_tracked': len(weak_refs),
                'pre_gc_surviving': pre_gc_surviving,
                'post_gc_surviving': post_gc_surviving,
                'gc_effectiveness': (pre_gc_surviving - post_gc_surviving) / pre_gc_surviving if pre_gc_surviving > 0 else 0,
                'corruption_detected': corruption_detected,
                'graph_nodes_created': 600,  # 100 + 200 + 300
                'class_instances_created': 200,  # 50 parents + 150 children
                'generators_created': 20,
                'circular_refs_stable': corruption_detected < 5
            }

        except Exception as e:
            return {'error': str(e), 'traceback': traceback.format_exc()}

    def test_high_frequency_allocation_cycles(self) -> Dict[str, Any]:
        """Test 4: High-frequency allocation/deallocation cycles"""
        print("\n‚ö° Testing High-Frequency Allocation Cycles...")

        initial_memory = self.get_memory_stats()
        allocation_times = []
        deallocation_times = []
        memory_errors = 0
        allocation_patterns = []

        try:
            print("  ‚Ä¢ Running extreme allocation/deallocation cycles...")

            # Test 4.1: Rapid small object allocation
            print("    ‚Ä¢ Rapid small object cycles...")

            for cycle in range(self.allocation_cycles):
                # Allocate various object types
                objects = []

                # Lists
                for i in range(10):
                    obj = [f'cycle_{cycle}_item_{i}'] * random.randint(5, 20)
                    objects.append(obj)

                # Dictionaries
                for i in range(5):
                    obj = {f'key_{j}': j for j in range(random.randint(5, 15))}
                    objects.append(obj)

                # Strings
                for i in range(20):
                    obj = ''.join(random.choices(string.ascii_letters, k=random.randint(10, 50)))
                    objects.append(obj)

                # Arrays
                for i in range(3):
                    obj = array.array('i', range(random.randint(10, 30)))
                    objects.append(obj)

                # Deques
                for i in range(2):
                    obj = collections.deque(range(random.randint(5, 25)))
                    objects.append(obj)

                # Track allocation time
                allocation_start = time.time()

                # Trigger deallocation
                del objects
                gc.collect()  # Force collection every cycle

                deallocation_end = time.time()

                # Store timing data
                allocation_patterns.append({
                    'cycle': cycle,
                    'objects_allocated': 40,  # Fixed count for this test
                    'allocation_time_ms': (deallocation_end - allocation_start) * 1000
                })

                # Periodic memory check
                if cycle % 1000 == 0:
                    current_memory = self.get_memory_stats()
                    if current_memory['rss_mb'] > initial_memory['rss_mb'] + 1000:  # 1GB growth limit
                        print(f"    ‚ö†Ô∏è  Memory growth detected at cycle {cycle}")
                        break

            # Test 4.2: Burst allocation patterns
            print("    ‚Ä¢ Burst allocation patterns...")

            burst_results = []
            for burst_size in [100, 500, 1000, 2000]:
                try:
                    burst_start = time.time()
                    burst_objects = []

                    for i in range(burst_size):
                        # Create medium-sized objects
                        obj = {
                            'id': i,
                            'data': list(range(50)),
                            'string': 'burst_data_' * 10,
                            'metadata': {'created': time.time(), 'size': 50}
                        }
                        burst_objects.append(obj)

                    burst_creation_time = time.time() - burst_start

                    # Rapid deallocation
                    burst_dealloc_start = time.time()
                    del burst_objects
                    gc.collect()
                    burst_dealloc_time = time.time() - burst_dealloc_start

                    burst_results.append({
                        'burst_size': burst_size,
                        'creation_time_ms': burst_creation_time * 1000,
                        'deallocation_time_ms': burst_dealloc_time * 1000,
                        'total_time_ms': (burst_creation_time + burst_dealloc_time) * 1000
                    })

                except MemoryError:
                    memory_errors += 1
                    burst_results.append({
                        'burst_size': burst_size,
                        'error': 'MemoryError'
                    })

            # Test 4.3: Memory pressure allocation
            print("    ‚Ä¢ Allocation under memory pressure...")

            pressure_objects = []
            pressure_allocations = 0
            pressure_errors = 0

            # Create memory pressure first
            for i in range(100):
                pressure_obj = ['pressure_data'] * 10000
                pressure_objects.append(pressure_obj)

            # Now try to allocate under pressure
            for i in range(1000):
                try:
                    test_obj = {'pressure_test': i, 'data': [j for j in range(100)]}
                    pressure_allocations += 1

                    # Simulate some work
                    _ = sum(test_obj['data'])

                    del test_obj

                except MemoryError:
                    pressure_errors += 1
                    break

            # Clean up pressure objects
            del pressure_objects
            gc.collect()

            # Test 4.4: Fragmentation resistance
            print("    ‚Ä¢ Testing fragmentation resistance...")

            fragment_objects = []
            allocation_sizes = []

            for i in range(1000):
                # Vary allocation sizes to cause fragmentation
                size_patterns = [10, 100, 1000, 5000, 10000, 50000, 100000]
                size = random.choice(size_patterns)

                try:
                    obj = ['frag'] * size
                    fragment_objects.append(obj)
                    allocation_sizes.append(size)

                    # Random deallocation to create holes
                    if random.random() < 0.3 and fragment_objects:
                        del fragment_objects[random.randint(0, len(fragment_objects) - 1)]

                except MemoryError:
                    memory_errors += 1
                    break

            # Test allocation efficiency
            avg_alloc_time = sum(pattern['allocation_time_ms'] for pattern in allocation_patterns) / len(allocation_patterns)

            # Test memory recovery
            mid_test_memory = self.get_memory_stats()

            # Final cleanup
            del fragment_objects
            gc.collect()

            final_memory = self.get_memory_stats()
            memory_recovered = mid_test_memory['rss_mb'] - final_memory['rss_mb']

            return {
                'initial_rss_mb': initial_memory['rss_mb'],
                'mid_test_rss_mb': mid_test_memory['rss_mb'],
                'final_rss_mb': final_memory['rss_mb'],
                'memory_recovered_mb': memory_recovered,
                'allocation_cycles_completed': len(allocation_patterns),
                'avg_allocation_time_ms': avg_alloc_time,
                'burst_tests_completed': len(burst_results),
                'pressure_allocations': pressure_allocations,
                'pressure_errors': pressure_errors,
                'fragmentation_allocations': len(allocation_sizes),
                'memory_errors': memory_errors,
                'fragmentation_resistant': memory_recovered > 50,  # Recovered at least 50MB
                'allocation_efficient': avg_alloc_time < 1.0,  # Less than 1ms per cycle
                'burst_results': burst_results
            }

        except Exception as e:
            return {'error': str(e), 'traceback': traceback.format_exc()}

    def test_extreme_race_conditions(self) -> Dict[str, Any]:
        """Test 5: Extreme concurrent memory access race conditions"""
        print("\nüèÅ Testing Extreme Race Conditions...")

        initial_memory = self.get_memory_stats()
        race_results = {}
        data_corruption_count = 0
        deadlock_count = 0

        try:
            print("  ‚Ä¢ Testing concurrent memory modification race conditions...")

            # Test 5.1: Shared data race conditions
            shared_data = {'counter': 0, 'list': [], 'dict': {}}
            race_errors = []

            def concurrent_modifier(thread_id, modifications, results):
                """Modify shared data to create race conditions"""
                local_errors = 0
                local_corruptions = 0

                for i in range(modifications):
                    try:
                        # Counter increment (classic race condition)
                        old_value = shared_data['counter']
                        time.sleep(0.001)  # Small delay to increase race probability
                        shared_data['counter'] = old_value + 1

                        # List operations
                        if len(shared_data['list']) > 100:
                            shared_data['list'].pop(0)
                        shared_data['list'].append(f'thread_{thread_id}_item_{i}')

                        # Dict operations
                        key = f'thread_{thread_id}_{i}'
                        shared_data['dict'][key] = {
                            'thread': thread_id,
                            'iteration': i,
                            'timestamp': time.time()
                        }

                        # Corruption check
                        if shared_data['counter'] < old_value:
                            local_corruptions += 1

                    except Exception:
                        local_errors += 1

                results.append({
                    'thread_id': thread_id,
                    'errors': local_errors,
                    'corruptions': local_corruptions,
                    'final_counter_read': shared_data['counter']
                })

            # Run concurrent modifiers without locks (intentional races)
            threads_results = []
            with ThreadPoolExecutor(max_workers=self.concurrent_threads) as executor:
                futures = []

                for i in range(self.concurrent_threads):
                    future = executor.submit(concurrent_modifier, i, 50, threads_results)
                    futures.append(future)

                # Wait with timeout to detect deadlocks
                try:
                    for future in as_completed(futures, timeout=10):
                        future.result()
                except TimeoutError:
                    deadlock_count += 1

            # Analyze race results
            total_expected = self.concurrent_threads * 50
            actual_counter = shared_data['counter']
            race_condition_detected = actual_counter != total_expected

            race_results['shared_data_race'] = {
                'expected_counter': total_expected,
                'actual_counter': actual_counter,
                'race_detected': race_condition_detected,
                'corruption_count': sum(r['corruptions'] for r in threads_results),
                'error_count': sum(r['errors'] for r in threads_results)
            }

            # Test 5.2: Memory allocation race conditions
            print("    ‚Ä¢ Testing memory allocation race conditions...")

            allocation_races = []
            shared_memory_pool = []

            def memory_allocator(thread_id, allocations):
                """Allocate memory concurrently to create allocation races"""
                local_allocations = []

                for i in range(allocations):
                    try:
                        # Allocate varying sizes
                        size = random.randint(100, 5000)
                        obj = [f'thread_{thread_id}_alloc_{i}'] * size

                        # Share some objects
                        if i % 5 == 0:
                            shared_memory_pool.append(obj)
                        else:
                            local_allocations.append(obj)

                        # Simulate some processing
                        total = sum(len(x) for x in local_allocations[-10:])

                        # Random deallocation
                        if i % 10 == 9 and local_allocations:
                            del local_allocations[random.randint(0, len(local_allocations) - 1)]

                    except MemoryError:
                        break

                return local_allocations

            # Run concurrent memory allocators
            with ThreadPoolExecutor(max_workers=10) as executor:
                futures = [executor.submit(memory_allocator, i, 20) for i in range(10)]

                allocation_results = []
                try:
                    for future in as_completed(futures, timeout=15):
                        result = future.result()
                        allocation_results.append(len(result))
                except TimeoutError:
                    deadlock_count += 1

            race_results['memory_allocation_race'] = {
                'threads_run': len(allocation_results),
                'total_allocations': sum(allocation_results),
                'shared_pool_size': len(shared_memory_pool),
                'deadlock_detected': deadlock_count > 0
            }

            # Test 5.3: Reference counting race conditions
            print("    ‚Ä¢ Testing reference counting race conditions...")

            ref_objects = []
            ref_corruption = 0

            class RefTestObject:
                def __init__(self, value):
                    self.value = value
                    self.ref_count = 0
                    self.data = list(range(100))

                def increment_ref(self):
                    self.ref_count += 1

                def decrement_ref(self):
                    self.ref_count = max(0, self.ref_count - 1)

            # Create shared reference objects
            for i in range(100):
                obj = RefTestObject(i)
                ref_objects.append(obj)

            def ref_modifier(thread_id, operations):
                """Modify reference counts concurrently"""
                local_corruption = 0

                for i in range(operations):
                    obj = ref_objects[i % len(ref_objects)]

                    try:
                        # Modify reference count
                        old_ref = obj.ref_count
                        obj.increment_ref()

                        # Simulate work
                        time.sleep(0.0001)

                        obj.decrement_ref()

                        # Check for corruption
                        if obj.ref_count < 0:
                            local_corruption += 1

                    except Exception:
                        local_corruption += 1

                return local_corruption

            # Run reference modifiers
            with ThreadPoolExecutor(max_workers=15) as executor:
                futures = [executor.submit(ref_modifier, i, 100) for i in range(15)]

                ref_corruption_results = []
                for future in as_completed(futures):
                    ref_corruption_results.append(future.result())

            total_ref_corruption = sum(ref_corruption_results)

            # Test 5.4: Data structure race conditions
            print("    ‚Ä¢ Testing data structure race conditions...")

            # Test concurrent modifications on same data structures
            test_list = []
            test_dict = {}
            test_set = set()

            structure_corruption = 0

            def structure_modifier(structure_type, operations):
                """Modify data structure concurrently"""
                local_corruption = 0

                for i in range(operations):
                    try:
                        if structure_type == 'list':
                            test_list.append(f'item_{i}')
                            if len(test_list) > 100:
                                test_list.pop(0)

                            # Verify list integrity
                            if len(test_list) > len(set(test_list)) * 2:  # Too many duplicates might indicate corruption
                                local_corruption += 1

                        elif structure_type == 'dict':
                            test_dict[f'key_{i}'] = f'value_{i}'
                            if len(test_dict) > 50:
                                del test_dict[list(test_dict.keys())[0]]

                        elif structure_type == 'set':
                            test_set.add(f'item_{i}')
                            if len(test_set) > 100:
                                test_set.pop()

                    except Exception:
                        local_corruption += 1

                return local_corruption

            # Run structure modifiers
            with ThreadPoolExecutor(max_workers=9) as executor:
                futures = [
                    executor.submit(structure_modifier, 'list', 100),
                    executor.submit(structure_modifier, 'dict', 100),
                    executor.submit(structure_modifier, 'set', 100)
                ]

                structure_results = []
                for future in as_completed(futures):
                    structure_results.append(future.result())

            total_structure_corruption = sum(structure_results)

            final_memory = self.get_memory_stats()
            memory_growth = final_memory['rss_mb'] - initial_memory['rss_mb']

            return {
                'initial_rss_mb': initial_memory['rss_mb'],
                'final_rss_mb': final_memory['rss_mb'],
                'memory_growth_mb': memory_growth,
                'race_condition_results': race_results,
                'data_corruption_count': data_corruption_count,
                'deadlock_count': deadlock_count,
                'reference_count_corruption': total_ref_corruption,
                'structure_corruption': total_structure_corruption,
                'concurrent_threads': self.concurrent_threads,
                'shared_data_integrity': not race_condition_detected,
                'memory_stable_under_concurrency': memory_growth < 200,  # Less than 200MB growth
                'race_conditions_present': True,  # We expect some race conditions
                'system_resilient': data_corruption_count < 10
            }

        except Exception as e:
            return {'error': str(e), 'traceback': traceback.format_exc()}

    def test_memory_corruption_detection(self) -> Dict[str, Any]:
        """Test 6: Memory corruption detection and resilience"""
        print("\nüîç Testing Memory Corruption Detection...")

        initial_memory = self.get_memory_stats()
        corruption_tests = {}

        try:
            print("  ‚Ä¢ Testing memory corruption patterns...")

            # Test 6.1: Buffer overflow simulation
            print("    ‚Ä¢ Buffer overflow simulation...")

            buffer_overflow_tests = []

            for test_size in [1000, 10000, 100000]:
                try:
                    # Create array and attempt boundary access
                    test_array = array.array('i', range(test_size))

                    corruption_detected = 0

                    # Test boundary conditions
                    valid_indices = [0, test_size // 2, test_size - 1]
                    for idx in valid_indices:
                        try:
                            value = test_array[idx]
                            test_array[idx] = value + 1
                        except IndexError:
                            corruption_detected += 1

                    # Test near-boundary access (should not corrupt)
                    boundary_tests = []
                    for offset in [-1, test_size]:
                        try:
                            _ = test_array[offset]
                            boundary_tests.append({'offset': offset, 'error': False})
                        except IndexError:
                            boundary_tests.append({'offset': offset, 'error': True})

                    buffer_overflow_tests.append({
                        'array_size': test_size,
                        'corruption_detected': corruption_detected,
                        'boundary_tests_passed': all(t['error'] for t in boundary_tests),
                        'integrity_maintained': True
                    })

                except Exception as e:
                    buffer_overflow_tests.append({
                        'array_size': test_size,
                        'error': str(e),
                        'integrity_maintained': False
                    })

            # Test 6.2: String corruption detection
            print("    ‚Ä¢ String corruption detection...")

            string_corruption_tests = []

            for string_size in [10000, 100000, 1000000]:
                try:
                    test_string = 'A' * string_size
                    original_hash = hashlib.md5(test_string.encode()).hexdigest()

                    # Perform various operations
                    operations = []

                    # Slicing
                    sliced = test_string[100:-100]
                    operations.append({'operation': 'slice', 'success': len(sliced) == string_size - 200})

                    # Concatenation
                    concatenated = test_string + 'B' * 1000
                    operations.append({'operation': 'concat', 'success': len(concatenated) == string_size + 1000})

                    # Encoding/decoding
                    try:
                        encoded = test_string.encode('utf-8')
                        decoded = encoded.decode('utf-8')
                        new_hash = hashlib.md5(decoded.encode()).hexdigest()
                        operations.append({
                            'operation': 'encode_decode',
                            'success': new_hash == original_hash
                        })
                    except Exception:
                        operations.append({'operation': 'encode_decode', 'success': False})

                    # Verify integrity after operations
                    final_string = test_string
                    final_hash = hashlib.md5(final_string.encode()).hexdigest()

                    string_corruption_tests.append({
                        'string_size': string_size,
                        'original_hash': original_hash,
                        'final_hash': final_hash,
                        'integrity_maintained': final_hash == original_hash,
                        'operations_success': sum(op['success'] for op in operations) / len(operations),
                        'operations': operations
                    })

                except Exception as e:
                    string_corruption_tests.append({
                        'string_size': string_size,
                        'error': str(e),
                        'integrity_maintained': False
                    })

            # Test 6.3: Object reference corruption
            print("    ‚Ä¢ Object reference corruption detection...")

            class CorruptionTestObject:
                def __init__(self, id):
                    self.id = id
                    self.data = list(range(100))
                    self.nested = {'inner_data': list(range(50))}
                    self.checksum = None
                    self.calculate_checksum()

                def calculate_checksum(self):
                    data_str = f"{self.id}{len(self.data)}{len(self.nested['inner_data'])}"
                    self.checksum = hashlib.md5(data_str.encode()).hexdigest()

                def verify_integrity(self):
                    self.calculate_checksum()
                    return self.checksum == self.calculate_checksum()

            reference_tests = []

            for obj_count in [100, 500, 1000]:
                objects = [CorruptionTestObject(i) for i in range(obj_count)]
                original_checksums = [obj.checksum for obj in objects]

                # Perform operations that might cause corruption
                for i, obj in enumerate(objects):
                    # Modify data
                    if i % 2 == 0:
                        obj.data.append(i)
                    if i % 3 == 0:
                        obj.nested['inner_data'].append(i)
                    if i % 5 == 0:
                        obj.id = obj.id * 2

                # Verify integrity
                corruption_count = 0
                for i, obj in enumerate(objects):
                    if not obj.verify_integrity():
                        corruption_count += 1

                reference_tests.append({
                    'object_count': obj_count,
                    'corruption_detected': corruption_count,
                    'integrity_rate': (obj_count - corruption_count) / obj_count,
                    'objects_modified': obj_count
                })

            # Test 6.4: Memory mapping corruption
            print("    ‚Ä¢ Memory mapping corruption test...")

            mmap_tests = []

            with tempfile.NamedTemporaryFile(delete=False) as tmp:
                try:
                    # Write test data
                    test_data = b'A' * (1024 * 1024)  # 1MB
                    tmp.write(test_data)
                    tmp.flush()

                    # Memory map the file
                    with open(tmp.name, 'r+b') as f:
                        mmapped_data = mmap.mmap(f.fileno(), 0)

                        # Test memory integrity
                        original_hash = hashlib.md5(mmapped_data[:1024]).hexdigest()

                        # Modify mapped memory
                        mmapped_data[0:10] = b'MODIFIED'

                        # Verify changes
                        modified_hash = hashlib.md5(mmapped_data[:1024]).hexdigest()

                        # Test boundary access
                        boundary_tests = []
                        try:
                            _ = mmapped_data[len(mmapped_data) - 1]
                            boundary_tests.append(True)
                        except IndexError:
                            boundary_tests.append(False)

                        mmap_tests.append({
                            'file_size_mb': len(mmapped_data) / (1024 * 1024),
                            'original_hash': original_hash,
                            'modified_hash': modified_hash,
                            'hash_different': original_hash != modified_hash,
                            'boundary_access': boundary_tests,
                            'corruption_detected': False
                        })

                        mmapped_data.close()

                    os.unlink(tmp.name)

                except Exception as e:
                    mmap_tests.append({
                        'error': str(e),
                        'corruption_detected': True
                    })

            # Test 6.5: Memory exhaustion recovery
            print("    ‚Ä¢ Memory exhaustion recovery test...")

            exhaustion_objects = []
            exhaustion_recovery = False
            max_objects_before_exhaustion = 0

            try:
                # Try to exhaust memory
                for i in range(10000):
                    large_obj = ['exhaustion_test'] * 100000
                    exhaustion_objects.append(large_obj)
                    max_objects_before_exhaustion = i + 1

                    # Check memory pressure
                    current_memory = self.get_memory_stats()
                    if current_memory['available_mb'] < 100:  # Less than 100MB available
                        break

            except MemoryError:
                # Expected at some point
                pass

            # Test recovery
            try:
                del exhaustion_objects
                gc.collect()

                # Try to allocate new objects after cleanup
                recovery_test = []
                for i in range(100):
                    recovery_test.append(['recovery'] * 1000)

                exhaustion_recovery = len(recovery_test) == 100
                del recovery_test

            except Exception:
                exhaustion_recovery = False

            final_memory = self.get_memory_stats()
            memory_growth = final_memory['rss_mb'] - initial_memory['rss_mb']

            return {
                'initial_rss_mb': initial_memory['rss_mb'],
                'final_rss_mb': final_memory['rss_mb'],
                'memory_growth_mb': memory_growth,
                'buffer_overflow_tests': buffer_overflow_tests,
                'string_corruption_tests': string_corruption_tests,
                'reference_integrity_tests': reference_tests,
                'memory_mapping_tests': mmap_tests,
                'exhaustion_test': {
                    'max_objects_before_exhaustion': max_objects_before_exhaustion,
                    'recovery_successful': exhaustion_recovery
                },
                'corruption_resilience': all(test.get('integrity_maintained', True) for test in buffer_overflow_tests),
                'system_stable': memory_growth < 1000  # Less than 1GB growth acceptable
            }

        except Exception as e:
            return {'error': str(e), 'traceback': traceback.format_exc()}

    def run_extreme_stress_test(self):
        """Run all extreme memory stress tests"""
        print("üî• EXTREME Memory Stress Test Agent")
        print("=" * 60)
        print(f"Test Duration: {self.test_duration} seconds")
        print(f"Process PID: {os.getpid()}")
        print(f"Initial Memory: {self.get_memory_stats()['rss_mb']:.1f}MB")
        print("=" * 60)

        start_time = time.time()
        test_start_memory = self.get_memory_stats()

        try:
            # Run extreme tests
            self.results['deep_nesting'] = self.test_extreme_deep_nesting()

            if time.time() - start_time < self.test_duration - 2:
                self.results['extreme_strings'] = self.test_extreme_string_operations()

            if time.time() - start_time < self.test_duration - 2:
                self.results['complex_circular_refs'] = self.test_complex_circular_references()

            if time.time() - start_time < self.test_duration - 2:
                self.results['allocation_cycles'] = self.test_high_frequency_allocation_cycles()

            if time.time() - start_time < self.test_duration - 2:
                self.results['race_conditions'] = self.test_extreme_race_conditions()

            if time.time() - start_time < self.test_duration - 2:
                self.results['memory_corruption'] = self.test_memory_corruption_detection()

        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Test interrupted by user")
        except Exception as e:
            print(f"\n‚ùå Test error: {e}")
            self.results['error'] = str(e)

        end_time = time.time()
        test_end_memory = self.get_memory_stats()
        total_runtime = end_time - start_time

        # Final cleanup
        print("\nüßπ Performing final cleanup...")
        gc.collect()

        final_cleanup_memory = self.get_memory_stats()

        # Generate comprehensive extreme test report
        self.generate_extreme_test_report(
            test_start_memory,
            test_end_memory,
            final_cleanup_memory,
            total_runtime
        )

    def generate_extreme_test_report(self, start_mem, end_mem, cleanup_mem, runtime):
        """Generate comprehensive extreme test report"""
        print("\n" + "=" * 60)
        print("üî• EXTREME MEMORY STRESS TEST REPORT")
        print("=" * 60)

        print(f"‚è±Ô∏è  Total Runtime: {runtime:.2f} seconds")
        print(f"üìä Start Memory: {start_mem['rss_mb']:.1f}MB")
        print(f"üìä Peak Memory: {end_mem['rss_mb']:.1f}MB")
        print(f"üìä Final Memory: {cleanup_mem['rss_mb']:.1f}MB")
        print(f"üìä Memory Growth: {end_mem['rss_mb'] - start_mem['rss_mb']:.1f}MB")
        print(f"üìä Memory Recovered: {end_mem['rss_mb'] - cleanup_mem['rss_mb']:.1f}MB")

        print("\nüìã EXTREME TEST RESULTS:")

        # Deep Nesting Results
        nesting_results = self.results.get('deep_nesting', {})
        if 'error' not in nesting_results:
            print(f"\nüèóÔ∏è  EXTREME Deep Nesting:")
            print(f"   ‚Ä¢ Max Depth Achieved: {nesting_results.get('max_nesting_depth', 0):,} levels")
            print(f"   ‚Ä¢ Nested Structures: {nesting_results.get('nested_structures_created', 0):,}")
            print(f"   ‚Ä¢ Memory Growth: {nesting_results.get('memory_growth_mb', 0):.1f}MB")
            print(f"   ‚Ä¢ Corruption Detected: {nesting_results.get('corruption_detected', 0)}")
            print(f"   ‚Ä¢ Deep Nesting Successful: {'‚úÖ YES' if nesting_results.get('deep_nesting_successful') else '‚ùå NO'}")

        # Extreme Strings Results
        strings_results = self.results.get('extreme_strings', {})
        if 'error' not in strings_results:
            print(f"\nüìù EXTREME String Operations:")
            print(f"   ‚Ä¢ Base String Size: {strings_results.get('base_string_size_mb', 0):.1f}MB")
            print(f"   ‚Ä¢ Concatenation Ops: {strings_results.get('concatenation_operations', 0):,}")
            print(f"   ‚Ä¢ Avg Concatenation Time: {strings_results.get('avg_concatenation_time_ms', 0):.2f}ms")
            print(f"   ‚Ä¢ Memory Recovered: {strings_results.get('memory_recovered_mb', 0):.1f}MB")
            print(f"   ‚Ä¢ String Integrity: {'‚úÖ MAINTAINED' if strings_results.get('string_operations_successful') else '‚ùå CORRUPTED'}")

        # Complex Circular References Results
        circular_results = self.results.get('complex_circular_refs', {})
        if 'error' not in circular_results:
            print(f"\nüîÑ Complex Circular References:")
            print(f"   ‚Ä¢ Circular Objects: {circular_results.get('circular_objects_created', 0):,}")
            print(f"   ‚Ä¢ GC Effectiveness: {circular_results.get('gc_effectiveness', 0):.1%}")
            print(f"   ‚Ä¢ Corruption Detected: {circular_results.get('corruption_detected', 0)}")
            print(f"   ‚Ä¢ System Stable: {'‚úÖ STABLE' if circular_results.get('circular_refs_stable') else '‚ùå UNSTABLE'}")

        # High-Frequency Allocation Results
        allocation_results = self.results.get('allocation_cycles', {})
        if 'error' not in allocation_results:
            print(f"\n‚ö° High-Frequency Allocation:")
            print(f"   ‚Ä¢ Allocation Cycles: {allocation_results.get('allocation_cycles_completed', 0):,}")
            print(f"   ‚Ä¢ Avg Cycle Time: {allocation_results.get('avg_allocation_time_ms', 0):.3f}ms")
            print(f"   ‚Ä¢ Memory Recovered: {allocation_results.get('memory_recovered_mb', 0):.1f}MB")
            print(f"   ‚Ä¢ Fragmentation Resistant: {'‚úÖ YES' if allocation_results.get('fragmentation_resistant') else '‚ùå NO'}")

        # Race Conditions Results
        race_results = self.results.get('race_conditions', {})
        if 'error' not in race_results:
            print(f"\nüèÅ Extreme Race Conditions:")
            print(f"   ‚Ä¢ Concurrent Threads: {race_results.get('concurrent_threads', 0)}")
            print(f"   ‚Ä¢ System Resilient: {'‚úÖ RESILIENT' if race_results.get('system_resilient') else '‚ùå VULNERABLE'}")
            print(f"   ‚Ä¢ Memory Stable: {'‚úÖ STABLE' if race_results.get('memory_stable_under_concurrency') else '‚ùå UNSTABLE'}")
            print(f"   ‚Ä¢ Deadlocks: {race_results.get('deadlock_count', 0)}")

        # Memory Corruption Results
        corruption_results = self.results.get('memory_corruption', {})
        if 'error' not in corruption_results:
            print(f"\nüîç Memory Corruption Detection:")
            print(f"   ‚Ä¢ Corruption Resilience: {'‚úÖ RESILIENT' if corruption_results.get('corruption_resilience') else '‚ùå VULNERABLE'}")
            print(f"   ‚Ä¢ System Stable: {'‚úÖ STABLE' if corruption_results.get('system_stable') else '‚ùå UNSTABLE'}")

            exhaustion_test = corruption_results.get('exhaustion_test', {})
            print(f"   ‚Ä¢ Exhaustion Recovery: {'‚úÖ RECOVERED' if exhaustion_test.get('recovery_successful') else '‚ùå FAILED'}")

        print("\n" + "=" * 60)
        print("üéØ EXTREME STRESS TEST ASSESSMENT:")

        # Calculate extreme scores
        extreme_scores = {}

        # Deep nesting score
        if 'deep_nesting' in self.results and 'error' not in self.results['deep_nesting']:
            nesting = self.results['deep_nesting']
            nesting_score = (
                (nesting.get('max_nesting_depth', 0) / 2500) * 0.6 +  # 60% depth
                (1 - nesting.get('corruption_detected', 0) / 100) * 0.4  # 40% integrity
            )
            extreme_scores['deep_nesting'] = min(1.0, max(0.0, nesting_score))

        # String operations score
        if 'extreme_strings' in self.results and 'error' not in self.results['extreme_strings']:
            strings = self.results['extreme_strings']
            string_score = (
                (1 if strings.get('string_operations_successful') else 0) * 0.7 +  # 70% integrity
                min(1.0, strings.get('memory_recovered_mb', 0) / 50) * 0.3  # 30% recovery
            )
            extreme_scores['string_operations'] = min(1.0, max(0.0, string_score))

        # Circular references score
        if 'complex_circular_refs' in self.results and 'error' not in self.results['complex_circular_refs']:
            circular = self.results['complex_circular_refs']
            circular_score = (
                circular.get('gc_effectiveness', 0) * 0.5 +  # 50% GC effectiveness
                (1 if circular.get('circular_refs_stable') else 0) * 0.5  # 50% stability
            )
            extreme_scores['circular_references'] = min(1.0, max(0.0, circular_score))

        # Race conditions score
        if 'race_conditions' in self.results and 'error' not in self.results['race_conditions']:
            race = self.results['race_conditions']
            race_score = (
                (1 if race.get('system_resilient') else 0) * 0.6 +  # 60% resilience
                (1 if race.get('memory_stable_under_concurrency') else 0) * 0.4  # 40% stability
            )
            extreme_scores['race_conditions'] = min(1.0, max(0.0, race_score))

        # Corruption detection score
        if 'memory_corruption' in self.results and 'error' not in self.results['memory_corruption']:
            corruption = self.results['memory_corruption']
            corruption_score = (
                (1 if corruption.get('corruption_resilience') else 0) * 0.5 +  # 50% resilience
                (1 if corruption.get('system_stable') else 0) * 0.5  # 50% stability
            )
            extreme_scores['corruption_detection'] = min(1.0, max(0.0, corruption_score))

        if extreme_scores:
            overall_extreme_score = sum(extreme_scores.values()) / len(extreme_scores)
            print(f"   ‚Ä¢ Overall Extreme Score: {overall_extreme_score:.1%}")

            if overall_extreme_score >= 0.9:
                print("   ‚Ä¢ Status: üî• EXTREME - System handles pathological cases exceptionally well")
            elif overall_extreme_score >= 0.75:
                print("   ‚Ä¢ Status: üü¢ EXCELLENT - System very robust under extreme stress")
            elif overall_extreme_score >= 0.6:
                print("   ‚Ä¢ Status: üü° GOOD - System handles most extreme cases well")
            elif overall_extreme_score >= 0.4:
                print("   ‚Ä¢ Status: üü† FAIR - System struggles with some extreme cases")
            else:
                print("   ‚Ä¢ Status: üî¥ POOR - System fails under extreme stress")

            print(f"\n   ‚Ä¢ Individual Scores:")
            for category, score in extreme_scores.items():
                status = "üî•" if score >= 0.9 else "‚úÖ" if score >= 0.75 else "‚ö†Ô∏è" if score >= 0.6 else "‚ùå"
                print(f"     ‚Ä¢ {category.replace('_', ' ').title()}: {score:.1%} {status}")

        print(f"\nüìÅ Detailed extreme results saved to: /tmp/extreme_memory_stress_test_results.json")

        # Save detailed extreme results
        with open('/tmp/extreme_memory_stress_test_results.json', 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'runtime_seconds': runtime,
                'memory_start': start_mem,
                'memory_end': end_mem,
                'memory_cleanup': cleanup_mem,
                'results': self.results,
                'scores': extreme_scores
            }, f, indent=2)

        print("=" * 60)

if __name__ == "__main__":
    # Run the extreme memory stress test
    agent = ExtremeMemoryStressTestAgent()
    agent.run_extreme_stress_test()